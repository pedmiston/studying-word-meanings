---
title: "Studying word meanings through the eyes"
author: "Pierce Edmiston"
---

# How are word meanings represented in the brain?

<aside class="notes">
I study how word meanings are represented in the brain. Specifically what I’m interested in is how word meanings are represented differently than other types of information or meaning that we learn from the environment around us.
</aside>

```{r config, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
library(printr)
opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  dev = "png",
  fig.align = "center",
  fig.path = "figs/",
  fig.width = 6,
  fig.height = 4,
  cache = TRUE,
  cache.path = ".cache/"
)

library(magrittr)
library(png)
library(jpeg)
library(grid)

read_chunk("R/motivated-cues.R")
read_chunk("R/property-verification.R")
read_chunk("R/orientation-discrimination.R")

source("R/theme.R")
gp <- gpar(fontfamily = "Consolas", fontsize = 18)
```

<aside class="notes">
An example I use a lot is the canonical dog. How is the meaning of the word "dog" represented in the brain and how is it different from the other information we know about dogs?
</aside>

```{r dog, fig.cap = ""}
dog <- "static/img/dog.png" %>% readPNG %>% rasterGrob(x = 0.6, width = 0.5)
grid.newpage()
grid.draw(dog)
grid.text("dog -->", x = 0.3, gp = gp)
```

<aside class="notes">
You all know a lot about dogs: you know what they are and what they look like, and there are a few different ways I can get you to think about dogs. I can show you this picture of course but I can also just say the word “dog” and hearing this will activate at least some of your knowledge about dogs. But I don’t need to use language, I bet I can get you to think about dogs by playing you this sound.
</aside>

<div style="text-align: center;">
  <audio src="http://sapir.psych.wisc.edu/meri/bark.wav" controls>
</div>

# Different cues to the same concept

<aside class="notes">
So what’s the difference between a dog `<bark>` and the word “dog” when all I'm trying to do is get you to think about the concept or category of all dogs? What’s the difference between verbal and nonverbal cues for activating the same underlying knowledge?
</aside>

```{r example-cues, results = 'asis'}
data.frame(
  word = paste0("\"", c("dog", "cat", "chainsaw", "bowling ball"), "\""),
  sound = paste0("`<", c("bark", "meow", "revving", "crashing pins"), ">`")
)
```

<aside class="notes">
To test this, we compared two types of cues: verbal and nonverbal cues to arguably the same underlying concept, like the word "dog" and the sound of a dog `<bark>`.
</aside>

# Picture verification task

```{r picture-verification-task, engine = "dot", cache = FALSE}
digraph {
  rankdir=LR;
  node[fontname=Helvetica, width=1, fontsize=20];
  cue -> delay -> picture -> question;

  cue[image="static/img/sound.png", label="", shape=none];
  delay[shape=none];
  question[shape=none, label="Y / N ?"];
  picture[shape=square];
}
```

<aside class="notes">
In the experiment we had people complete what is called a picture verification task. If you were a subject in this experiment, here is what you would have done. You'd be holding a game controller, and on each trial you'd hear some sound, like the word "dog" or the sound of a dog `<bark>`, and then you'd see a picture, either a picture of a dog, or a picture of something else. And you'd have to decide, does what I see match what I heard--is this a picture of a dog, or is it a picture of something else? If they match you press one button, and if they don't, you press another button. And you do this 400 times. (Fun game!).
</aside>

# Label advantage

```{r label_advantage}
grid.newpage()
grid.draw("static/img/lupyan_thompsonschill_2012_jepg/fig_1a.png" %>% readPNG %>% rasterGrob(height = 0.8))
grid.text("Lupyan & Thompson-Schill 2012 JEP:G", y = 0.95, gp = gp)
```

<aside class="notes">
From some previous work by my advisor Gary Lupyan and Sharon Thompson-Schill we know that people are overall faster at this task when cued with words than when cued with nonverbal sounds. And this isn't just a product of not being given enough time to process the nonverbal sounds. If you give people more time between the cue and the picture, the label advantage actually gets a bit bigger. And it isn't due to just having more experience with words than the sounds either. People have no problem naming the sounds when they are played all by themselves, and when you make people learn new words and new sounds for "aliens and alien musical instruments" you still see some advantages of words over their nonverbal equivalents.
</aside>

# Words are unmotivated cues

```{r sound-picture-congruence}
pictures <- c("guitar_acoustic", "guitar_electric") %>%
  lapply(function (x) paste0(x, ".jpg") %>% file.path("static/img", .) %>% readJPEG)
grid.newpage()
grid.draw(rasterGrob(pictures[[1]], x = 0.10, width = 0.5))
grid.draw(rasterGrob(pictures[[2]], x = 0.90, width = 0.5))
grid.text("< sound picture congruence >", gp = gp)
```

<aside class="notes">
So why are words better then sounds? One thing we've argued is that words are **unmotivated cues**. Thinking back to some of the categories we are talking about (e.g., dog, cat, chainsaw, bowling ball), different pictures of these objects can vary in degree to which they resemble the source of the sound.

These guitars are a good example. I can say the word "guitar" and you don't know which guitar I'm talking about. It's unmotivated. However, if I play you the sound of one of these guitars, you are pretty likely to be able to identify which one it came from. Nonverbal sounds are motivated to refer to a particular source.
</aside>

<div style="text-align: center">
  <audio src="http://sapir.psych.wisc.edu/meri/acoustic_guitar.wav" controls>
  <br />
  <audio src="http://sapir.psych.wisc.edu/meri/electric_guitar.wav" controls>
</div>

<aside class="notes">
We can think about this as a dimension of **sound-picture congruence**. Basically it's the idea that when I hear a nonverbal sound, it has some physical source out there in the world, and we wanted to know whether having a particular source sort of constrained the meaning of the sound even when people are specifically told to treat all guitars the same.
</aside>

##

```{r motivated-cues-exp1}
```

<aside class="notes">
So we ran a version of the sound picture verification task where we varied the congruence between the sound and the picture, and even though people were encouraged to ignore this information, it turns out people are slower to match a sound to an incongruent picture--an acoustic guitar sound to an electric guitar picture--than they match those same images based on words.

What's also interesting is that the label advantage exists even on congruent trials where the sound is arguably a better cue to the particular picture than even the word. In the next experiment we tried to make the label advantage even smaller by playing the sounds at the same time as the pictures, basically to imitate the real world experience of hearing these sounds as much as possible.
</aside>

##

```{r picture-verification-task, engine = "dot"}
```

```{r picture-verification-task-simultaneous, engine = "dot"}
digraph {
  rankdir=LR;
  fontname=Helvetica;
  label="Simultaneous";
  labelloc=t;

  node[fontname=Helvetica, width=1, fontsize=20];

  picture -> question;
  cue -> invis[style=invis];

  cue[image="static/img/sound.png", label="", shape=none];
  invis[style=invis];
  question[shape=none, label="Y / N ?"];
  picture[shape=square];
}
```

<aside class="notes">
So we ran another study where on some trials we presented the cue at the same time as the picture.
</aside>

##

```{r motivated-cues-exp2}
```

<aside class="notes">
And we found that we could make the label advantage disappear under two conditions: first, the sound and the picture had to match. That's the measure of sound congruence in the plot. Second, the sound and the picture had to be played at the same time.
</aside>

# So, word meanings are symbolic. Big whoop!

<aside class="notes">
What we've shown is that one of the ways word meanings are different from other types of meaning is that they are more symbolic and categorical, and they don't have to refer to a particular member of a category, they can just refer to the whole category, and this is advantageous when you're doing things like deciding whether or not a picture shows a member of a particular category. This much is relatively uncontroversial (although it is a bit surprising that people can't suppress this information when doing this simple task).

But what I'm going to show you next is going to challenge this view. I'm going to show you a way in which at least some word meanings are not symbolic, and they are dependent on individual experiences.
</aside>

# Property verification

<aside class="notes">
For the next experiment we had people complete a similar sort of task in that people are responding yes or no, but instead of verifying a picture they are verifying some property of an object. So you might be asked whether or not an alligator has a long tail.
</aside>

```{r property-verification-trial-structure}
trial_structure <- readPNG("static/img/property-verification-trial-structure.png") %>%
  rasterGrob
grid.newpage()
grid.draw(trial_structure)
```

<aside class="notes">
So what we are interested in here is whether the knowledge that you use to answer this question, although it doesn't explicitly require visualizing an actual alligator or swan or whatever, whether it still requires visual mechanisms. If it does, we should be able to disrupt your ability to answer these questions by presenting visual interference while you are answering the questions.
</aside>

#

##

<aside class="notes">
Here is a sample trial. First you'll get a question, then they'll be a delay, and then you are given an object, and just decide for yourself as quickly as you can whether or not the answer is yes or no.
</aside>

<div style="text-align: center">
  <video src="http://sapir.psych.wisc.edu/meri/big-teeth-tiger-no-mask.mov" controls></video>
</div>

##

<aside class="notes">
Here's another trial, and this shows the visual interference manipulation.
</aside>

<div style="text-align: center">
  <video src="http://sapir.psych.wisc.edu/meri/longneck-swan-with-mask.mov" controls></video>
</div>

# Property verification

```{r property-verification-results}
```

<aside class="notes">
And here are the results. So what we found was that when you were asked about the encyclopedic properties of the objects, the visual interference had no effect. If you were asked whether alligators lived in Florida, this information could not be disrupted by visual interference. If however you were asked whether alligators had big teeth or about some other visual property of the object, if the visual interference was present you were more likely to make an error.

What this means is that the knowledge of what things look like is represented at least in part in a visual format--something that can be disrupted simply by looking at something that is visually distracting.
</aside>

# Orientation discrimination

##

```{r orientation-discrimination-trial-structure}
trial_structure <- readPNG("static/img/orientation-discrimination-trial-structure.png") %>%
  rasterGrob
grid.newpage()
grid.draw(trial_structure)
```

<aside class="notes">
</aside>

##

```{r orientation-discrimination-exp1}
```

<aside class="notes">
</aside>

# A paradox?
